{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10543431,"sourceType":"datasetVersion","datasetId":6523696}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install chromadb\n%pip install langchain-community ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport transformers\nimport pandas as pd\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.schema import Document\nimport numpy as np\nimport torch\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import RetrievalQA\nfrom typing import List, Dict, Any, Optional\nfrom langchain.schema import BaseRetriever\nimport re\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MedicalDataProcessor:\n    def __init__(self, csv_path):\n        self.df = pd.read_csv(csv_path)\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50,\n            length_function=len,\n            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n        )\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name=\"dmis-lab/biobert-base-cased-v1.2\",\n            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n        )\n    \n    def prepare_documents(self):\n        documents = []\n        for _, row in self.df.iterrows():\n            content = f\"Question: {row['Questions']}\\nAnswer: {row['Answers']}\"\n            \n            # Update metadata to match your CSV columns\n            doc = Document(\n                page_content=content,\n                metadata={\n                    'focus': row['Focus'],\n                    'question': row['Questions'],\n                    'source': 'MedQuAD'\n                }\n            )\n            documents.append(doc)\n        return documents\n     \n    def create_chunks(self, documents):\n        \"\"\"Split documents into chunks.\"\"\"\n        return self.text_splitter.split_documents(documents)\n    \n    def create_vectorstore(self, chunks, persist_directory=\"medical_vectorstore\"):\n        \"\"\"Create and persist vector store.\"\"\"\n        vectorstore = Chroma.from_documents(\n            documents=chunks,\n            embedding=self.embeddings,\n            persist_directory=persist_directory\n        )\n        vectorstore.persist()\n        return vectorstore\n\ndef main():\n    # Initialize processor\n    processor = MedicalDataProcessor(\"/kaggle/input/medquad-processed/processed_medquad.csv\")\n    # Create documents with metadata\n    print(\"Preparing documents...\")\n    documents = processor.prepare_documents()\n    \n    # Split into chunks\n    print(\"Creating chunks...\")\n    chunks = processor.create_chunks(documents)\n    \n    # Create vector store\n    print(\"Creating vector store...\")\n    vectorstore = processor.create_vectorstore(chunks)\n    \n    print(f\"Processing complete. Total chunks created: {len(chunks)}\")\n    return vectorstore\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:07:42.422038Z","iopub.execute_input":"2025-02-01T20:07:42.422515Z","iopub.status.idle":"2025-02-01T20:15:58.260765Z","shell.execute_reply.started":"2025-02-01T20:07:42.422487Z","shell.execute_reply":"2025-02-01T20:15:58.259785Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-5-f5f9e5c8e029>:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  self.embeddings = HuggingFaceEmbeddings(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ace6d3ff6f84930979d76fbf5fb7532"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cd781d02cf94872b0ac9f54303466b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d58fd5a18042b485585a2f7ca12109"}},"metadata":{}},{"name":"stdout","text":"Preparing documents...\nCreating chunks...\nCreating vector store...\nProcessing complete. Total chunks created: 68377\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-5-f5f9e5c8e029>:43: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n  vectorstore.persist()\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-02-02T00:04:28.554226Z","iopub.execute_input":"2025-02-02T00:04:28.554540Z","iopub.status.idle":"2025-02-02T00:04:28.560073Z","shell.execute_reply.started":"2025-02-02T00:04:28.554511Z","shell.execute_reply":"2025-02-02T00:04:28.559244Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"download_file('/kaggle/working/medical_vectostore', 'out')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T00:07:38.994137Z","iopub.execute_input":"2025-02-02T00:07:38.994434Z","iopub.status.idle":"2025-02-02T00:07:39.002281Z","shell.execute_reply.started":"2025-02-02T00:07:38.994408Z","shell.execute_reply":"2025-02-02T00:07:39.001433Z"}},"outputs":[{"name":"stdout","text":"Unable to run zip command!\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from pydantic import Field\nfrom langchain.schema import BaseRetriever\nfrom typing import List, Any\n\nclass SimilarQuestionRetriever(BaseRetriever):\n    vectorstore: Any = Field(default=None, description=\"Vector store for document retrieval\")\n    \n    def _get_relevant_documents(self, query: str) -> List[Document]:\n        # Get the most similar document\n        docs_and_scores = self.vectorstore.similarity_search_with_score(query, k=1)\n        \n        if not docs_and_scores:\n            return []\n        \n        most_similar_doc, _ = docs_and_scores[0]\n        original_question = most_similar_doc.metadata.get('question', '')\n        \n        # Get documents with the same question and their answers\n        similar_docs = self.vectorstore.similarity_search(\n            original_question,\n            k=2,\n            filter={\"question\": original_question}\n        )\n        \n        return similar_docs\n\n    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n        return self._get_relevant_documents(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:08:34.538954Z","iopub.execute_input":"2025-02-01T21:08:34.539381Z","iopub.status.idle":"2025-02-01T21:08:34.546835Z","shell.execute_reply.started":"2025-02-01T21:08:34.539357Z","shell.execute_reply":"2025-02-01T21:08:34.545944Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MedicalRAGSystem:\n    def __init__(self, model_path=\"BioMistral/BioMistral-7B\", persist_directory=\"medical_vectorstore\"):\n        self.setup_model(model_path)\n        self.load_vectorstore(persist_directory)\n        self.setup_prompt()\n        self.setup_qa_chain()\n\n    def setup_model(self, model_path):\n        \"\"\"Initialize BioMistral model and tokenizer.\"\"\"\n        print(\"Loading BioMistral model and tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        \n        pipeline = transformers.pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            max_length=512,  # Back to max_length for better control\n            temperature=0.1,  # Reduced for more focused responses\n            do_sample=False,  # Deterministic output\n            top_p=0.95,\n            repetition_penalty=1.1  # Prevent repetition\n        )\n        \n        self.llm = HuggingFacePipeline(pipeline=pipeline)\n        print(\"Model setup complete!\")\n\n    def load_vectorstore(self, persist_directory):\n        \"\"\"Load existing vector store with similar question retrieval.\"\"\"\n        print(\"Loading vector store...\")\n        self.vectorstore = Chroma(\n            persist_directory=persist_directory,\n            embedding_function=HuggingFaceEmbeddings(\n                model_name=\"dmis-lab/biobert-base-cased-v1.2\",\n                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n            )\n        )\n        self.retriever = SimilarQuestionRetriever(vectorstore=self.vectorstore)\n        print(\"Vector store loaded!\")\n\n    def setup_prompt(self):\n        \"\"\"Create enhanced prompt template for medical QA.\"\"\"\n        template = \"\"\"[INST] You are a medical assistant answering health-related questions.\n        Use only the following medical information to answer the question.\n        If no relevant information is found in the context, respond with:\n        \"I apologize, but I don't have any information about [topic] in my knowledge base.\"\n        \n        Medical Knowledge:\n        {context}\n        \n        Question: {question}\n        \n        Please provide a clear, accurate medical response using only the information from the Medical Knowledge section above. [/INST]\"\"\"\n        \n        self.prompt = PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"]\n        )\n\n    def setup_qa_chain(self):\n        \"\"\"Setup the question-answering chain.\"\"\"\n        print(\"Setting up QA chain...\")\n        self.qa_chain = RetrievalQA.from_chain_type(\n            llm=self.llm,\n            chain_type=\"stuff\",\n            retriever=self.retriever,\n            chain_type_kwargs={\n                \"prompt\": self.prompt\n            },\n            return_source_documents=True\n        )\n        print(\"QA chain ready!\")\n\n    def get_answer(self, question: str):\n        \"\"\"Get answer for a medical question.\"\"\"\n        try:\n            result = self.qa_chain({\"query\": question})\n            return {\n                \"answer\": result[\"result\"],\n                \"sources\": [\n                    {\n                        \"focus\": doc.metadata.get(\"focus\", \"\"),\n                        \"question\": doc.metadata.get(\"question\", \"\")\n                    } \n                    for doc in result[\"source_documents\"]\n                ]\n            }\n        except Exception as e:\n            return {\n                \"answer\": \"I encountered an error processing your question. Please try again.\",\n                \"error\": str(e)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:08:37.946785Z","iopub.execute_input":"2025-02-01T21:08:37.947154Z","iopub.status.idle":"2025-02-01T21:08:37.956804Z","shell.execute_reply.started":"2025-02-01T21:08:37.947126Z","shell.execute_reply":"2025-02-01T21:08:37.955926Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"rag_system = MedicalRAGSystem()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:08:42.068243Z","iopub.execute_input":"2025-02-01T21:08:42.068555Z","iopub.status.idle":"2025-02-01T21:09:15.265221Z","shell.execute_reply.started":"2025-02-01T21:08:42.068534Z","shell.execute_reply":"2025-02-01T21:09:15.264203Z"}},"outputs":[{"name":"stdout","text":"Loading BioMistral model and tokenizer...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n<ipython-input-5-ab98325c58e6>:29: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  self.llm = HuggingFacePipeline(pipeline=pipeline)\n<ipython-input-5-ab98325c58e6>:37: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embedding_function=HuggingFaceEmbeddings(\n","output_type":"stream"},{"name":"stdout","text":"Model setup complete!\nLoading vector store...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-5-ab98325c58e6>:35: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n  self.vectorstore = Chroma(\n","output_type":"stream"},{"name":"stdout","text":"Vector store loaded!\nSetting up QA chain...\nQA chain ready!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def test_medical_qa(question: str, rag_system):\n    print(\"Question:\", question)\n    print(\"\\nProcessing...\")\n    response = rag_system.get_answer(question)\n    \n    if \"error\" in response:\n        print(\"\\nError:\", response[\"error\"])\n        return\n        \n    answer = response.get(\"answer\", \"\")\n    if \"[INST]\" in answer:\n        answer = answer.split(\"[/INST]\")[-1].strip()\n    print(\"\\nAnswer:\", answer)\n    \n    if \"sources\" in response:\n        print(\"\\nSources:\")\n        for source in response[\"sources\"]:\n            print(f\"\\nFocus: {source['focus']}\")\n            print(f\"Original Question: {source['question']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:40:20.226599Z","iopub.execute_input":"2025-02-01T20:40:20.226987Z","iopub.status.idle":"2025-02-01T20:40:20.232643Z","shell.execute_reply.started":"2025-02-01T20:40:20.226957Z","shell.execute_reply":"2025-02-01T20:40:20.231758Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"test_medical_qa(\"How to diagnose Schimke immunoosseous dysplasia ?\",rag_system)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:40:22.622165Z","iopub.execute_input":"2025-02-01T20:40:22.622543Z","iopub.status.idle":"2025-02-01T20:40:31.730035Z","shell.execute_reply.started":"2025-02-01T20:40:22.622515Z","shell.execute_reply":"2025-02-01T20:40:31.729261Z"}},"outputs":[{"name":"stdout","text":"Question: How to diagnose Schimke immunoosseous dysplasia ?\n\nProcessing...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-6-ab98325c58e6>:81: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  result = self.qa_chain({\"query\": question})\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAnswer: Schimke immunodysplasia is a rare autosomal recessive disorder characterized by short stature, skeletal dysplasia, renal failure, T-cell deficiency, and thyroid dysfunction. Other features include shallow acetabular roofs, spondyloepiphyseal dysplasia, thoracic kyphosis, and a waddling gait. Diagnosis is based on clinical presentation and genetic testing.\n\nSources:\n\nFocus: Schimke immunoosseous dysplasia\nOriginal Question: What are the symptoms of Schimke immunoosseous dysplasia ?\n\nFocus: Schimke immunoosseous dysplasia\nOriginal Question: What are the symptoms of Schimke immunoosseous dysplasia ?\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%pip install ragas sentence-transformers\n%pip install ragas langchain-google-genai google-generativeai","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_recall,\n    context_precision\n)\nfrom datasets import Dataset\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:08:06.121686Z","iopub.execute_input":"2025-02-01T21:08:06.122024Z","iopub.status.idle":"2025-02-01T21:08:09.150540Z","shell.execute_reply.started":"2025-02-01T21:08:06.121993Z","shell.execute_reply":"2025-02-01T21:08:09.149885Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class RAGEvaluator:\n    def __init__(self, rag_system, test_data_path=\"/kaggle/input/medquad-processed/processed_medquad.csv\"):\n        self.rag_system = rag_system\n        self.test_df = pd.read_csv(test_data_path)\n        self.llm = ChatGoogleGenerativeAI(\n            model=\"gemini-pro\",\n            temperature=0.1,\n            google_api_key=\"YOUR_API_KEY\"\n        )\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n        )\n    \n    def prepare_evaluation_data(self, num_samples=10):\n        \"\"\"Prepare dataset for RAGAS evaluation\"\"\"\n        eval_data = []\n        \n        # Sample questions\n        sampled_data = self.test_df.sample(n=num_samples, random_state=42)\n        \n        for _, row in sampled_data.iterrows():\n            # Get RAG response\n            response = self.rag_system.get_answer(row['Questions'])\n            \n            # Clean up answer\n            answer = response[\"answer\"]\n            if \"[INST]\" in answer:\n                answer = answer.split(\"[/INST]\")[-1].strip()\n            \n            # Format contexts properly for RAGAS\n            contexts = []\n            for doc in response['sources']:\n                context = f\"{doc['question']}: {doc['focus']}\"\n                if len(context) > 500:\n                    context = context[:500] + \"...\"\n                contexts.append(context)\n            \n            # Truncate reference if too long\n            reference = row['Answers']\n            if len(reference) > 1000:\n                reference = reference[:1000] + \"...\"\n                \n            data_point = {\n                \"question\": row['Questions'],\n                \"answer\": answer,\n                \"contexts\": contexts,\n                \"reference\": reference,\n            }\n            \n            eval_data.append(data_point)\n        \n        dataset = Dataset.from_list(eval_data)\n        return dataset\n\n    def run_evaluation(self, num_samples=10):\n        \"\"\"Run RAGAS evaluation\"\"\"\n        print(\"Preparing evaluation dataset...\")\n        eval_dataset = self.prepare_evaluation_data(num_samples)\n        \n        print(\"\\nRunning RAGAS evaluation...\")\n        results = evaluate(\n            eval_dataset,\n            metrics=[\n                faithfulness,\n                answer_relevancy,\n                context_recall,\n                context_precision\n            ],\n            llm=self.llm,\n            embeddings=self.embeddings,\n            raise_exceptions=True\n        )\n        \n        return results\n\n    def print_results(self, results):\n        \"\"\"Print evaluation results in a readable format\"\"\"\n        print(\"\\nRAGAS Evaluation Results:\")\n        print(\"-\" * 50)\n        print(f\"Results type: {type(results)}\")\n        print(f\"Results content: {results}\")\n        \n        if isinstance(results, list):\n            for i, result in enumerate(results):\n                print(f\"\\nResult {i}:\")\n                print(f\"Type: {type(result)}\")\n                print(f\"Dir: {dir(result)}\")\n                try:\n                    # Try to access common attributes\n                    if hasattr(result, 'name'):\n                        print(f\"Name: {result.name}\")\n                    if hasattr(result, 'score'):\n                        print(f\"Score: {result.score:.3f}\")\n                    if hasattr(result, 'metadata'):\n                        print(f\"Metadata: {result.metadata}\")\n                except Exception as e:\n                    print(f\"Error accessing result attributes: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:15:26.780482Z","iopub.execute_input":"2025-02-01T21:15:26.780919Z","iopub.status.idle":"2025-02-01T21:15:26.791889Z","shell.execute_reply.started":"2025-02-01T21:15:26.780887Z","shell.execute_reply":"2025-02-01T21:15:26.790933Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"evaluator = RAGEvaluator(rag_system)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:15:30.759140Z","iopub.execute_input":"2025-02-01T21:15:30.759433Z","iopub.status.idle":"2025-02-01T21:15:32.765652Z","shell.execute_reply.started":"2025-02-01T21:15:30.759412Z","shell.execute_reply":"2025-02-01T21:15:32.764740Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"results = evaluator.run_evaluation(num_samples=5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T21:21:52.734803Z","iopub.execute_input":"2025-02-01T21:21:52.735207Z","iopub.status.idle":"2025-02-01T21:24:30.891136Z","shell.execute_reply.started":"2025-02-01T21:21:52.735170Z","shell.execute_reply":"2025-02-01T21:24:30.890172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = evaluator.run_evaluation(num_samples=3)\n# Print formatted results\nevaluator.print_results(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}